{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4a7e5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"csv_to_parquet_landing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "40c71689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script execution for raw data to landing\n",
    "class CSV_to_parquet:\n",
    "    \n",
    "    \"\"\"Script to read raw file from local path to landing..\"\"\"\n",
    "    \n",
    "    def __init__(self,file_parameter):\n",
    "        \n",
    "        \"\"\"function to get input parameters to read the file from local path\"\"\"\n",
    "        self.file_parameter=file_parameter\n",
    "        \n",
    "        \"\"\"read raw data from local\"\"\"\n",
    "    def read_csv(self,fileformat,file_path):\n",
    "        return spark.read.format(fileformat).option(\"header\",\"True\").load(self.file_parameter['src_base_read_path']+file_path)\n",
    "        \n",
    "        \"\"\"save file as parquet and also save as table in local db\"\"\"\n",
    "    def write_parquet(self,df,dest_path,table_name):  \n",
    "        print(dest_path)\n",
    "        df.write.mode(\"overwrite\").parquet(self.file_parameter['dest_base_write_path'])\n",
    "#         spark.read.parquet(self.file_parameter['dest_base_write_path']).show()\n",
    "\n",
    "#         df.write.format(self.file_parameter['parquet']).mode(\"overwrite\").option(\"path\",self.file_parameter['dest_base_write_path'])\\\n",
    "#         .saveAsTable(\"school_landing.{0}\".format(table_name))\n",
    "        #spark.sql(\"select * FROM school_landing.{0}\".format(table_name)).show(df.count(),truncate=False)\n",
    "        \n",
    "        \"\"\"creating temporary view to add audit columns- created timestamp,creator name,updator name\"\"\"\n",
    "    def create_view(self,df,schemas):\n",
    "        \n",
    "        df1 = spark.createDataFrame(data=df.rdd,schema=schemas)\n",
    "        #df1.show()\n",
    "        df1.createOrReplaceTempView(self.file_parameter['temp_table'])\n",
    "        temp_df = spark.sql(\"select * from {0}\".format(self.file_parameter['temp_table']))\n",
    "        df__audit_user=temp_df.withColumn(self.file_parameter['audit_created_col_name'],lit(self.file_parameter['audit_created_username'])) \\\n",
    "        .withColumn(self.file_parameter['audit_created_timestamp'],current_timestamp()) \\\n",
    "        .withColumn(self.file_parameter['audit_updated_col_name'],lit(self.file_parameter['audit_updated_username'])) \\\n",
    "        .withColumn(self.file_parameter['audit_updated_timestamp'],current_timestamp())\n",
    "        #df__audit_user.show(df__audit_user.count(),truncate=False)\n",
    "        return df__audit_user\n",
    "    \n",
    "\n",
    "    \n",
    "    def main_block(self):\n",
    "        \n",
    "        marks_schema = StructType([ \\\n",
    "            StructField(\"exam_date\",StringType(),True), \\\n",
    "            StructField(\"marks\",StringType(),True), \\\n",
    "            StructField(\"student_id\",StringType(),True), \\\n",
    "            StructField(\"subject_id\",StringType(),True), \\\n",
    "            ])\n",
    "        students_schema = StructType([ \\\n",
    "            StructField(\"student_name\",StringType(),True), \\\n",
    "            StructField(\"student_id\",StringType(),True), \\\n",
    "          \n",
    "            ])\n",
    "        subjects_schema = StructType([ \\\n",
    "             StructField(\"subject\",StringType(),True), \\\n",
    "            StructField(\"subject_id\",StringType(),True), \\\n",
    "            ])        \n",
    "        \"\"\"main execution block\"\"\"\n",
    "        spark.sql(\"create database if not exists {0}\".format(self.file_parameter['local_db_name']))\n",
    "        df1=self.read_csv(self.file_parameter['csv'],self.file_parameter['read_filename_marks']);\n",
    "        parquet_df1 = self.create_view(df1,marks_schema);\n",
    "        self.write_parquet(parquet_df1,self.file_parameter['dest_base_write_path']+self.file_parameter['write_filename_marks'],self.file_parameter['table_name_marks']);\n",
    "        \n",
    "#         df2=self.read_csv(self.file_parameter['csv'],self.file_parameter['read_filename_student']);\n",
    "#         parquet_df2 = self.create_view(df2,students_schema);\n",
    "#         self.write_parquet(parquet_df2, self.file_parameter['dest_base_write_path']+self.file_parameter['write_filename_student'],self.file_parameter['table_name_students']);\n",
    "        \n",
    "#         df3=self.read_csv(self.file_parameter['csv'],self.file_parameter['read_filename_subject'],);\n",
    "#         parquet_df3 = self.create_view(df3,students_schema);\n",
    "#         self.write_parquet(parquet_df3,self.file_parameter['dest_base_write_path']+self.file_parameter['write_filename_subject'],self.file_parameter['table_name_subject']);\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e5a7bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/school_landingmarkdetails.parquet\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    file_read={\n",
    "        \"csv_format\":\".csv\",\n",
    "        \"csv\":\"csv\",\n",
    "        \"parquet_format\":\".parquet\",\n",
    "        \"parquet\":\"parquet\",\n",
    "        \"table_name_marks\":\"marks\",\n",
    "        \"table_name_students\":\"students\",\n",
    "        \"table_name_subject\":\"subjects\",\n",
    "        \"read_filename_marks\":\"/marks/markdetails.csv\",\n",
    "        \"read_filename_student\":\"/student/studentdetails.csv\",\n",
    "        \"read_filename_subject\":\"/subject/subjectdetails.csv\",\n",
    "        \"write_filename_marks\":\"/marks/markdetails.parquet\",\n",
    "        \"write_filename_student\":\"/student/studentdetails.parquet\",\n",
    "        \"write_filename_subject\":\"/subject/subjectdetails.parquet\",\n",
    "        \"src_base_read_path\":\"/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/pre_landing/raw_files/school\",\n",
    "        \"dest_base_write_path\":\"/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/\",\n",
    "        \"src_path\":\"/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/pre_landing/raw_files/school/marks/markdetails.csv\",\n",
    "        \"dest_path\":\"/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet\",\n",
    "        \"temp_table\":\"school_table\",\n",
    "        \"local_db_name\":\"school_landing\",\n",
    "        \"audit_created_username\":\"vasanth\",\n",
    "        \"audit_created_col_name\":\"audit_created_username\",\n",
    "        \"audit_created_timestamp\":\"audit_created_timestamp\",\n",
    "        \"audit_updated_username\":\"vasanth\",\n",
    "        \"audit_updated_col_name\":\"audit_updated_username\",\n",
    "        \"audit_updated_timestamp\":\"audit_updated_timestamp\",\n",
    "\n",
    "    }\n",
    "   \n",
    "    \n",
    "\n",
    "    prelanding_obj1=CSV_to_parquet(file_read)\n",
    "    prelanding_obj1.main_block()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bafb68d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_0/w1rr1_0s03jflrd55q7nsxjx83h730/T/ipykernel_28830/903748818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Vasanth/spark-apache/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    299\u001b[0m                        int96RebaseMode=int96RebaseMode)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
      "\u001b[0;32m~/Vasanth/spark-apache/spark-3.2.0-bin-hadoop3.2/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Vasanth/spark-apache/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/vasanth_ku/Vasanth/spark-apache/git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
