{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4a7e5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"csv_to_parquet_landing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "40c71689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script execution for raw data to landing\n",
    "class CSV_to_parquet:\n",
    "    \n",
    "    \"\"\"Script to read raw file from local path to landing..\"\"\"\n",
    "    \n",
    "    def __init__(self,file_parameter):\n",
    "        \n",
    "        \"\"\"function to get input parameters to read the file from local path\"\"\"\n",
    "        self.file_parameter=file_parameter\n",
    "        \n",
    "        \"\"\"read raw data from local\"\"\"\n",
    "    def read_csv(self,fileformat,file_path):\n",
    "        return spark.read.format(fileformat).option(\"header\",\"True\").load(self.file_parameter['src_base_read_path']+file_path)\n",
    "        \n",
    "        \"\"\"save file as parquet and also save as table in local db\"\"\"\n",
    "    def write_parquet(self,df,dest_path,table_name):  \n",
    "        df.write.mode(\"overwrite\").parquet(dest_path)\n",
    "        df.write.format(self.file_parameter['parquet']).mode(\"overwrite\").option(\"path\",dest_path)\\\n",
    "        .saveAsTable(\"school_landing.{0}\".format(table_name))\n",
    "        spark.read.parquet(dest_path).show()\n",
    "\n",
    "        #spark.sql(\"select * FROM school_landing.{0}\".format(table_name)).show(df.count(),truncate=False)\n",
    "        \n",
    "        \"\"\"creating temporary view to add audit columns- created timestamp,creator name,updator name\"\"\"\n",
    "    def create_view(self,df,schemas):\n",
    "        \n",
    "        df1 = spark.createDataFrame(data=df.rdd,schema=schemas)\n",
    "        #df1.show()\n",
    "        df1.createOrReplaceTempView(self.file_parameter['temp_table'])\n",
    "        temp_df = spark.sql(\"select * from {0}\".format(self.file_parameter['temp_table']))\n",
    "        df__audit_user=temp_df.withColumn(self.file_parameter['audit_created_col_name'],lit(self.file_parameter['audit_created_username'])) \\\n",
    "        .withColumn(self.file_parameter['audit_created_timestamp'],current_timestamp()) \\\n",
    "        .withColumn(self.file_parameter['audit_updated_col_name'],lit(self.file_parameter['audit_updated_username'])) \\\n",
    "        .withColumn(self.file_parameter['audit_updated_timestamp'],current_timestamp())\n",
    "        #df__audit_user.show(df__audit_user.count(),truncate=False)\n",
    "        return df__audit_user\n",
    "    \n",
    "\n",
    "    \n",
    "    def main_block(self):\n",
    "        \n",
    "        marks_schema = StructType([ \\\n",
    "            StructField(\"exam_date\",StringType(),True), \\\n",
    "            StructField(\"marks\",StringType(),True), \\\n",
    "            StructField(\"student_id\",StringType(),True), \\\n",
    "            StructField(\"subject_id\",StringType(),True), \\\n",
    "            ])\n",
    "        students_schema = StructType([ \\\n",
    "            StructField(\"student_name\",StringType(),True), \\\n",
    "            StructField(\"student_id\",StringType(),True), \\\n",
    "          \n",
    "            ])\n",
    "        subjects_schema = StructType([ \\\n",
    "             StructField(\"subject_name\",StringType(),True), \\\n",
    "            StructField(\"subject_id\",StringType(),True), \\\n",
    "            ])        \n",
    "        \"\"\"main execution block\"\"\"\n",
    "        spark.sql(\"create database if not exists {0}\".format(self.file_parameter['local_db_name']))\n",
    "        df1=self.read_csv(self.file_parameter['csv'],self.file_parameter['read_filename_marks']);\n",
    "        df1.show()\n",
    "        parquet_df1 = self.create_view(df1,marks_schema);\n",
    "        self.write_parquet(parquet_df1,self.file_parameter['dest_base_write_path']+self.file_parameter['write_filename_marks'],self.file_parameter['table_name_marks']);\n",
    "        \n",
    "        df2=self.read_csv(self.file_parameter['csv'],self.file_parameter['read_filename_student']);\n",
    "        parquet_df2 = self.create_view(df2,students_schema);\n",
    "        self.write_parquet(parquet_df2, self.file_parameter['dest_base_write_path']+self.file_parameter['write_filename_student'],self.file_parameter['table_name_students']);\n",
    "        \n",
    "        df3=self.read_csv(self.file_parameter['csv'],self.file_parameter['read_filename_subject'],);\n",
    "        parquet_df3 = self.create_view(df3,subjects_schema);\n",
    "        self.write_parquet(parquet_df3,self.file_parameter['dest_base_write_path']+self.file_parameter['write_filename_subject'],self.file_parameter['table_name_subject']);\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e5a7bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+----+\n",
      "|11-26-2021| 79|1682|7001|\n",
      "+----------+---+----+----+\n",
      "|11-26-2021| 50|1682|7002|\n",
      "|11-26-2021| 79|1682|7003|\n",
      "|11-26-2021| 55|1682|7004|\n",
      "|12-26-2021| 71|1682|7001|\n",
      "|12-26-2021| 81|1682|7002|\n",
      "|12-26-2021| 92|1682|7003|\n",
      "|12-26-2021| 64|1682|7004|\n",
      "|01-26-2022| 64|1682|7001|\n",
      "|01-26-2022| 83|1682|7002|\n",
      "|01-26-2022| 64|1682|7003|\n",
      "|01-26-2022| 94|1682|7004|\n",
      "|11-26-2021| 84|2180|7001|\n",
      "|11-26-2021| 72|2180|7002|\n",
      "|11-26-2021| 89|2180|7003|\n",
      "|11-26-2021| 92|2180|7004|\n",
      "|12-26-2021| 95|2180|7001|\n",
      "|12-26-2021| 49|2180|7002|\n",
      "|12-26-2021| 93|2180|7003|\n",
      "|12-26-2021| 55|2180|7004|\n",
      "|01-26-2022| 50|2180|7001|\n",
      "+----------+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/07 18:40:44 WARN HadoopFSUtils: The directory file:/Users/vasanth_ku/Vasanth/spark-apache/git%20projects/big_data_project_git/git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----------+----------------------+-----------------------+----------------------+-----------------------+\n",
      "| exam_date|marks|student_id|subject_id|audit_created_username|audit_created_timestamp|audit_updated_username|audit_updated_timestamp|\n",
      "+----------+-----+----------+----------+----------------------+-----------------------+----------------------+-----------------------+\n",
      "|11-26-2021|   50|      1682|      7002|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|11-26-2021|   79|      1682|      7003|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|11-26-2021|   55|      1682|      7004|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   71|      1682|      7001|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   81|      1682|      7002|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   92|      1682|      7003|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   64|      1682|      7004|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|01-26-2022|   64|      1682|      7001|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|01-26-2022|   83|      1682|      7002|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|01-26-2022|   64|      1682|      7003|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|01-26-2022|   94|      1682|      7004|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|11-26-2021|   84|      2180|      7001|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|11-26-2021|   72|      2180|      7002|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|11-26-2021|   89|      2180|      7003|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|11-26-2021|   92|      2180|      7004|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   95|      2180|      7001|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   49|      2180|      7002|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   93|      2180|      7003|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|12-26-2021|   55|      2180|      7004|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "|01-26-2022|   50|      2180|      7001|               vasanth|   2022-02-07 18:40:...|               vasanth|   2022-02-07 18:40:...|\n",
      "+----------+-----+----------+----------+----------------------+-----------------------+----------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    file_read={\n",
    "        \"csv_format\":\".csv\",\n",
    "        \"csv\":\"csv\",\n",
    "        \"parquet_format\":\".parquet\",\n",
    "        \"parquet\":\"parquet\",\n",
    "        \"table_name_marks\":\"marks\",\n",
    "        \"table_name_students\":\"students\",\n",
    "        \"table_name_subject\":\"subjects\",\n",
    "        \"read_filename_marks\":\"/marks/markdetails.csv\",\n",
    "        \"read_filename_student\":\"/student/studentdetails.csv\",\n",
    "        \"read_filename_subject\":\"/subject/subjectdetails.csv\",\n",
    "        \"write_filename_marks\":\"/marks/markdetails.parquet\",\n",
    "        \"write_filename_student\":\"/student/studentdetails.parquet\",\n",
    "        \"write_filename_subject\":\"/subject/subjectdetails.parquet\",\n",
    "        \"src_base_read_path\":\"../../../../git projects/big_data_project_git/data_files/pre_landing/raw_files/school\",\n",
    "        \"dest_base_write_path\":\"../../../../git projects/big_data_project_git/data_files/landing/school_landing\",\n",
    "        \"src_path\":\"../../../../git projects/big_data_project_git/data_files/pre_landing/raw_files/school/marks/markdetails.csv\",\n",
    "        \"dest_path\":\"../../../../git projects/big_data_project_git/data_files/landing/school_landing/marks/markdetails.parquet\",\n",
    "        \"temp_table\":\"school_table\",\n",
    "        \"local_db_name\":\"school_landing\",\n",
    "        \"audit_created_username\":\"vasanth\",\n",
    "        \"audit_created_col_name\":\"audit_created_username\",\n",
    "        \"audit_created_timestamp\":\"audit_created_timestamp\",\n",
    "        \"audit_updated_username\":\"vasanth\",\n",
    "        \"audit_updated_col_name\":\"audit_updated_username\",\n",
    "        \"audit_updated_timestamp\":\"audit_updated_timestamp\",\n",
    "\n",
    "    }\n",
    "   \n",
    "    \n",
    "\n",
    "    landing_obj1=CSV_to_parquet(file_read)\n",
    "    landing_obj1.main_block()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb68d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
